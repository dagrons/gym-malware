import os
import random
import gym
from train_agent_chainer import create_acer_agent
import numpy as np
from gym_malware.envs.utils import interface_v2, pefeatures
from gym_malware.envs.controls import manipulate2 as manipulate

from gym_malware import sha256_train, sha256_holdout, MAXTURNS
from collections import defaultdict

from keras.models import load_model

ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

num_test_episode = 500


def evaluate(action_function):
    success = []
    misclassified = []
    success_mean_len = 0
    for _ in range(num_test_episode):
        action_len = 0
        sha256 = random.sample(sha256_holdout)
        success_dict = defaultdict(list)
        bytez = interface_v2.fetch_file(sha256)
        label = interface_v2.get_label_local(bytez)
        if label == 0.0:
            misclassified.append(sha256)
            continue  # already misclassified, move along
        for _ in range(MAXTURNS):
            action_len += 1
            action = action_function(bytez)
            print(action)
            success_dict[sha256].append(action)
            bytez = manipulate.modify_without_breaking(bytez, [action])
            new_label = interface_v2.get_label_local(bytez)
            if new_label == 0.0:
                success_mean_len += action_len
                success.append(success_dict)
                break
    success_mean_len /= len(success)
    # evasion accuracy is len(success) / len(sha256_holdout)
    return success, misclassified, success_mean_len


def get_latest_model_from(basedir):
    dirs = os.listdir(basedir)
    lastmodel = -1
    for d in dirs:
        try:
            if int(d) > lastmodel:
                lastmodel = int(d)
        except ValueError:
            continue

    assert lastmodel >= 0, "No saved models!"
    return os.path.join(basedir, str(lastmodel))


if __name__ == '__main__':
    # baseline: choose actions at random
    def random_action(bytez): return np.random.choice(
        list(manipulate.ACTION_TABLE.keys()))
    random_success, misclassified, random_success_mean_len = evaluate(
        random_action)
    # don't count misclassified towards success

    ENV_NAME = 'malware-test-v0'
    env = gym.make(ENV_NAME)

    fe = pefeatures.PEFeatureExtractor()

    def agent_policy(agent):
        def f(bytez):
            # first, get features from bytez
            feats = fe.extract(bytez)
            action_index = agent.act(feats)
            return ACTION_LOOKUP[action_index]
        return f

    # test for label agent
    agent = create_acer_agent(env)
    # pull latest stored model
    last_model_dir = get_latest_model_from('models/acer_chainer')
    agent.load(last_model_dir)
    success, _, label_success_mean_len = evaluate(agent_policy(agent))

    # test for score agent
    agent_score = create_acer_agent(env)
    # pull latest stored model
    last_model_dir = get_latest_model_from('models/acer_score_chainer')
    agent_score.load(last_model_dir)
    score_success, _, score_success_mean_len = evaluate(agent_policy(agent))

    print("Success rate of random chance: {}\n, Mean Len: {}".format(
        len(random_success) / total), random_success_mean_len)
    print("Success rate (black box): {}\n, Mean Len {}".format(
        len(success) / total), label_success_mean_len)
    print("Success rate (score): {}\n, Mean Len{}".format(
        len(score_success) / total), score_success_mean_len)
